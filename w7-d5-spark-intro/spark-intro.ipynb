{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Intro\n",
    "\n",
    "**Help Preparing the pyspark env:** https://github.com/boyander/runpyspark\n",
    "\n",
    "**Important**: You have to run this notebook using `pyspark` command.\n",
    "\n",
    "1. Create the `SparkSession`\n",
    "2. Open spark dashboard in  [\"http://localhost:4040\"](\"http://localhost:4040\") \n",
    "\n",
    "https://spark.apache.org/docs/latest/monitoring.html\n",
    "\n",
    "Every SparkContext launches a web UI, by default on port 4040, that displays useful information about the application. This includes:\n",
    "\n",
    "A list of scheduler stages and tasks\n",
    "A summary of RDD sizes and memory usage\n",
    "Environmental information.\n",
    "Information about the running executors\n",
    "You can access this interface by simply opening http://<driver-node>:4040 in a web browser. If multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040 (4041, 4042, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a SparkContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open Spark Dashboard in browser tab\n",
    "import webbrowser\n",
    "webbrowser.open(\"http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.1413392\n"
     ]
    }
   ],
   "source": [
    "# https://www.geogebra.org/m/cF7RwK3H\n",
    "import random\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "NUM_SAMPLES = 10000000\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "print(\"Pi is roughly {}\".format(4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Spark commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-------------+\n",
      "|      Date|    Time|Transaction|         Item|\n",
      "+----------+--------+-----------+-------------+\n",
      "|2016-10-30|09:58:11|          1|        Bread|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30|10:07:57|          3|          Jam|\n",
      "|2016-10-30|10:07:57|          3|      Cookies|\n",
      "|2016-10-30|10:08:41|          4|       Muffin|\n",
      "|2016-10-30|10:13:03|          5|       Coffee|\n",
      "|2016-10-30|10:13:03|          5|       Pastry|\n",
      "|2016-10-30|10:13:03|          5|        Bread|\n",
      "|2016-10-30|10:16:55|          6|    Medialuna|\n",
      "|2016-10-30|10:16:55|          6|       Pastry|\n",
      "|2016-10-30|10:16:55|          6|       Muffin|\n",
      "|2016-10-30|10:19:12|          7|    Medialuna|\n",
      "|2016-10-30|10:19:12|          7|       Pastry|\n",
      "|2016-10-30|10:19:12|          7|       Coffee|\n",
      "|2016-10-30|10:19:12|          7|          Tea|\n",
      "|2016-10-30|10:20:51|          8|       Pastry|\n",
      "|2016-10-30|10:20:51|          8|        Bread|\n",
      "|2016-10-30|10:21:59|          9|        Bread|\n",
      "+----------+--------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading a CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../data/breadbasket_dms.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------------------+\n",
      "|Transaction|  collect_list(Item)|approx_count_distinct(Item)|\n",
      "+-----------+--------------------+---------------------------+\n",
      "|       1090|[Brownie, Coffee,...|                          4|\n",
      "|       1159|             [Bread]|                          1|\n",
      "|       1436|      [Coffee, Soup]|                          2|\n",
      "|       1512|[Hearty & Seasona...|                          3|\n",
      "|       1572|    [Pastry, Coffee]|                          2|\n",
      "|       2069|             [Bread]|                          1|\n",
      "|       2088|      [Scandinavian]|                          1|\n",
      "|       2136|[Hot chocolate, Tea]|                          2|\n",
      "|       2162|[Coffee, Tea, Jui...|                          7|\n",
      "|       2294|         [Tea, NONE]|                          2|\n",
      "|       2904|[NONE, Sandwich, ...|                          4|\n",
      "|        296|[Farm House, Scan...|                          2|\n",
      "|       3210|      [Bread, Bread]|                          1|\n",
      "|       3414|            [Coffee]|                          1|\n",
      "|       3606|             [Bread]|                          1|\n",
      "|       3959|      [Scandinavian]|                          1|\n",
      "|       4032|               [Tea]|                          1|\n",
      "|        467|             [Bread]|                          1|\n",
      "|       4821|   [Sandwich, Bread]|                          2|\n",
      "|       4937|[Sandwich, Sandwi...|                          3|\n",
      "+-----------+--------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "Row(Transaction='1090', collect_list(Item)=['Brownie', 'Coffee', 'Hot chocolate', 'Coffee', 'Juice'], approx_count_distinct(Item)=4)\n",
      "CPU times: user 21.1 ms, sys: 6.72 ms, total: 27.8 ms\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Grouping operation with distinct count\n",
    "from pyspark.sql.functions import collect_list, approx_count_distinct\n",
    "\n",
    "q = df.groupby(df.Transaction).agg(collect_list(\"Item\"), approx_count_distinct(\"Item\"))\n",
    "print(q.show())\n",
    "print(q.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------------------+---------+---------------+--------+-----+\n",
      "|Transaction|Adjustment|Afternoon with the baker|Alfajores|Argentina Night|Art Tray|Bacon|\n",
      "+-----------+----------+------------------------+---------+---------------+--------+-----+\n",
      "|       4821|         0|                       0|        0|              0|       0|    0|\n",
      "|       9030|         0|                       0|        0|              0|       0|    0|\n",
      "|        296|         0|                       0|        0|              0|       0|    0|\n",
      "|       2904|         0|                       0|        0|              0|       0|    0|\n",
      "|       1572|         0|                       0|        0|              0|       0|    0|\n",
      "|       6613|         0|                       0|        0|              0|       0|    0|\n",
      "|       6194|         0|                       0|        1|              0|       0|    0|\n",
      "|       7273|         0|                       0|        0|              0|       0|    0|\n",
      "|       6731|         0|                       0|        0|              0|       0|    0|\n",
      "|       6240|         0|                       0|        0|              0|       0|    0|\n",
      "|       9586|         0|                       0|        0|              0|       0|    0|\n",
      "|       2294|         0|                       0|        0|              0|       0|    0|\n",
      "|       4937|         0|                       0|        0|              0|       0|    0|\n",
      "|       2088|         0|                       0|        0|              0|       0|    0|\n",
      "|       9009|         0|                       0|        0|              0|       0|    0|\n",
      "|       1436|         0|                       0|        0|              0|       0|    0|\n",
      "|       1090|         0|                       0|        0|              0|       0|    0|\n",
      "|        829|         0|                       0|        0|              0|       0|    0|\n",
      "|       3414|         0|                       0|        0|              0|       0|    0|\n",
      "|        467|         0|                       0|        0|              0|       0|    0|\n",
      "+-----------+----------+------------------------+---------+---------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_pivoted = df.groupBy(\"Transaction\").pivot(\"Item\").agg(F.lit(1)).na.fill(0)\n",
    "df_pivoted.select(df_pivoted.columns[:7]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Transaction', 'Adjustment', 'Afternoon with the baker', 'Alfajores', 'Argentina Night', 'Art Tray', 'Bacon', 'Baguette', 'Bakewell', 'Bare Popcorn', 'Basket', 'Bowl Nic Pitt', 'Bread', 'Bread Pudding', 'Brioche and salami', 'Brownie', 'Cake', 'Caramel bites', 'Cherry me Dried fruit', 'Chicken Stew', 'Chicken sand', 'Chimichurri Oil', 'Chocolates', 'Christmas common', 'Coffee', 'Coffee granules ', 'Coke', 'Cookies', 'Crepes', 'Crisps', 'Drinking chocolate spoons ', 'Duck egg', 'Dulce de Leche', 'Eggs', \"Ella's Kitchen Pouches\", 'Empanadas', 'Extra Salami or Feta', 'Fairy Doors', 'Farm House', 'Focaccia', 'Frittata', 'Fudge', 'Gift voucher', 'Gingerbread syrup', 'Granola', 'Hack the stack', 'Half slice Monster ', 'Hearty & Seasonal', 'Honey', 'Hot chocolate', 'Jam', 'Jammie Dodgers', 'Juice', 'Keeping It Local', 'Kids biscuit', 'Lemon and coconut', 'Medialuna', 'Mighty Protein', 'Mineral water', 'Mortimer', 'Muesli', 'Muffin', 'My-5 Fruit Shoot', 'NONE', 'Nomad bag', 'Olum & polenta', 'Panatone', 'Pastry', 'Pick and Mix Bowls', 'Pintxos', 'Polenta', 'Postcard', 'Raspberry shortbread sandwich', 'Raw bars', 'Salad', 'Sandwich', 'Scandinavian', 'Scone', 'Siblings', 'Smoothies', 'Soup', 'Spanish Brunch', 'Spread', 'Tacos/Fajita', 'Tartine', 'Tea', 'The BART', 'The Nomad', 'Tiffin', 'Toast', 'Truffles', 'Tshirt', \"Valentine's card\", 'Vegan Feast', 'Vegan mincepie', 'Victorian Sponge']\n",
      "96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Features=SparseVector(93, {9: 1.0, 72: 1.0})),\n",
       " Row(Features=SparseVector(93, {21: 1.0, 48: 1.0})),\n",
       " Row(Features=SparseVector(93, {35: 1.0, 73: 1.0})),\n",
       " Row(Features=SparseVector(93, {21: 1.0, 60: 1.0, 72: 1.0, 82: 1.0})),\n",
       " Row(Features=SparseVector(93, {21: 1.0, 64: 1.0}))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_pivoted = df_pivoted.withColumn(\"Transaction\", df_pivoted[\"Transaction\"].cast(IntegerType()))\n",
    "print(df_pivoted.columns[:])\n",
    "print(len(df_pivoted.columns))\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=df_pivoted.columns[3:], outputCol=\"Features\")\n",
    "new_df = vecAssembler.transform(df_pivoted)\n",
    "X = new_df.select('Features')\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "num_clusters = 4\n",
    "data = X.rdd.map(lambda x: x[0].toArray()) \n",
    "clusters = KMeans.train(data, num_clusters, maxIterations=15, initializationMode=\"random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4821</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Transaction  Label\n",
       "0         4821      3\n",
       "1         9030      0\n",
       "2          296      1\n",
       "3         2904      0\n",
       "4         1572      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Transaction': df_pivoted.select(\"Transaction\").toPandas()['Transaction'],\n",
    "    'Label': clusters.predict(X.rdd.map(lambda x: x[0].toArray())).collect()\n",
    "})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4519\n",
       "1    2728\n",
       "3    2230\n",
       "2      54\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
